{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dcbcf1c-fe3a-4089-b016-504378ed638e",
   "metadata": {},
   "source": [
    "# Implementation Of Simple GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6ae577-63bb-4303-8345-c66e37c2e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d53ef91-a357-4b02-8491-7378c33d1c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab_size = 0\n",
    "    def fit(self,texts):\n",
    "        vocab = set()\n",
    "        for text in texts:\n",
    "            word = text.split()\n",
    "            vocab.update(words)\n",
    "\n",
    "        self.word2idx = {word:idx for idx,word in enumerate(sorted(vocab))}\n",
    "        self.idx2word = {idx:word for word,idx in self.word2idx.items()}\n",
    "        self.vocab_size = len(vocab)\n",
    "    def encode(self,text):\n",
    "        return [self.word2idx[word] for word in text.split()]\n",
    "    def decode(self,indices):\n",
    "        return \"\".join([self.idx2word[idx] for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72752d47-946d-4e30-a024-7c17f7dd0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_length):\n",
    "        self.tokenzier = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.data = []\n",
    "        for text in texts:\n",
    "            tokens = self.tokenizer.encode(text)\n",
    "            for i in range(0,len(tokens)-seq_length):\n",
    "                self.data.append((\n",
    "                    tokens[i:i+seq_length],\n",
    "                    tokens[i+1:i+seq_length+1]\n",
    "                ))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        x,y = self.data[idx]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32707547-1788-4b65-b6fb-406af033df8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3369477223.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 22\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query -> q\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key -> k\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value -> v\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Query -> o\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask = None):\n",
    "        scores = torch.matmul(Q,K.transpose(-2,-1))/np.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention = F.softmax(scores, dim = -1)\n",
    "        return torch.matmul(attention, V)\n",
    "    def forward(self, x, mask = None):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        Q = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        K = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        V = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9d195-b844-465b-9458-809dc296f802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3a58ec-a2c8-4126-b2f4-964e906435c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
